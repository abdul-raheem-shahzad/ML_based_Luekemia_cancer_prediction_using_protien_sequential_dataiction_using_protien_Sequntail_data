{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> ***MACHINE LEARNING BASED LUEKAEMIA CANCER PREDICTION SYSTEM USING PROTEIN SEQUENTIAL DATA*** </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Note:***\n",
    "#### *The features are extracted from Di-peptide Compissiton Technique*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Table of Content***\n",
    "1. Importing Libraries.\n",
    "\n",
    "2. Collecting information about dataset.\n",
    "3. Spltting dataset into X and Y.\n",
    "4. Checking Co-realtion.\n",
    "5. Outliers Identification\n",
    "6. Outlier Removal.\n",
    "7. Null Value Checking.\n",
    "8. Dropping some Features.\n",
    "9. Filling NaN values\n",
    "10. Data Augmentation:\n",
    "\n",
    "    - Meringing X and Y dataset\n",
    "\n",
    "    - Then split into Positive and Negative Dataframes\n",
    "    - Perfroming Augmentation on Positive Dataset only\n",
    "    - Perfroming Augmentation on Negative Dataset only\n",
    "    - Merge the Postive and Negative & split into X and Y\n",
    "    \n",
    "\n",
    "11. Passing data through Machine Learning Algorithms\n",
    "\n",
    "    - SVM = 90% ~ 93%\n",
    "    \n",
    "    - Random Forest = 90% ~ 92%\n",
    "    - K Neighbour Classifier = 83% ~ 84%\n",
    "    - XG-Boost = 84% ~ 85%\n",
    "    - Decision Tree = 81% ~ 84%\n",
    "    - Logistic Regression = 66% ~ 67%\n",
    "    - ROC Curve for all the above algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Importing Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Reading Dataset csv file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('CML_PAAC_Combined_1.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Checking Shape of Dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Getting some information***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Splitting into Input & Output***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = df.iloc[:,0:25] \n",
    "Y_output = df.iloc[:,-1]\n",
    "print(f'Shape of X_input:{X_input.shape}\\nShape of Y_output:{Y_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Input data sample***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_input sample:')\n",
    "X_input.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Output data sample***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Y_output sample:')\n",
    "Y_output.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Checking Outliers***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=X_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=pd.melt(X_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Removing Outliers***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_threshold = X_input.quantile(0.95)\n",
    "min_threshold = X_input.quantile(0.01)\n",
    "X_input_removed_outliers = X_input[(X_input<max_threshold)&(X_input>min_threshold)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Results***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=X_input_removed_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=pd.melt(X_input_removed_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Checking Co-relation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr1=X_input.corr(method='pearson')\n",
    "fig, ax = plt.subplots(figsize=(10,10)) \n",
    "print(sns.heatmap(corr1, annot=True,linewidths=2,ax=ax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(X_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Now Checking Null values***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_checking=X_input_removed_outliers.isnull().sum()\n",
    "null_checking = null_checking.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Dropping Columns***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input_dropped = X_input_removed_outliers.drop(columns=['Var1_21','Var1_22','Var1_23','Var1_23','Var1_24','Var1_25'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Checking Mean values of each column***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=X_input_dropped.mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Filling Null values of each column with its mean***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input_new=X_input_dropped.fillna(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***After filling Null values:***\n",
    "#### - ***we will make sure there is no NaN values in dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input_new.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***DATA AUGMENTATION***\n",
    "\n",
    "### ***Merging dataset***\n",
    "- Merging the input and output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_merge=pd.concat([X_input_new,Y_output], axis=1) \n",
    "dataset_merge.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug=dataset_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Splitting Positive DATA SAMPLE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive=data_aug.drop(data_aug[(data_aug.Label==0)].index)\n",
    "#positive.drop('Label',inplace=True, axis=1)\n",
    "positive.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***DATA AUGMENTATION ON POSITIVE DATA SAMPLES***\n",
    "##### ***saving each column name with their standard deviation value***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1p=positive.Var1_1\n",
    "var2p=positive.Var1_2\n",
    "var3p=positive.Var1_3\n",
    "var4p=positive.Var1_4\n",
    "var5p=positive.Var1_5\n",
    "var6p=positive.Var1_6\n",
    "var7p=positive.Var1_7\n",
    "var8p=positive.Var1_8\n",
    "var9p=positive.Var1_9\n",
    "var10p=positive.Var1_10\n",
    "var11p=positive.Var1_11\n",
    "var12p=positive.Var1_12\n",
    "var13p=positive.Var1_13\n",
    "var14p=positive.Var1_14\n",
    "var15p=positive.Var1_15\n",
    "var16p=positive.Var1_16\n",
    "var17p=positive.Var1_17\n",
    "var18p=positive.Var1_18\n",
    "var19p=positive.Var1_19\n",
    "var20p=positive.Var1_20\n",
    "labelp=1\n",
    "##############\n",
    "vars1p=np.std(var1p)\n",
    "vars2p=np.std(var2p)\n",
    "vars3p=np.std(var3p)\n",
    "vars4p=np.std(var4p)\n",
    "vars5p=np.std(var5p)\n",
    "vars6p=np.std(var6p)\n",
    "vars7p=np.std(var7p)\n",
    "vars8p=np.std(var8p)\n",
    "vars9p=np.std(var9p)\n",
    "vars10p=np.std(var10p)\n",
    "vars11p=np.std(var11p)\n",
    "vars12p=np.std(var12p)\n",
    "vars13p=np.std(var13p)\n",
    "vars14p=np.std(var14p)\n",
    "vars15p=np.std(var15p)\n",
    "vars16p=np.std(var16p)\n",
    "vars17p=np.std(var17p)\n",
    "vars18p=np.std(var18p)\n",
    "vars19p=np.std(var19p)\n",
    "vars20p=np.std(var20p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***performing full data augmentation on POSITIVE samples***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_positive=[]\n",
    "for _,row in positive.iterrows():\n",
    "        temp={\n",
    "            'Var1_1':row['Var1_1'],\n",
    "            'Var1_2':row['Var1_2'],\n",
    "            'Var1_3':row['Var1_3'],\n",
    "            'Var1_4':row['Var1_4'],\n",
    "            'Var1_5':row['Var1_5'],\n",
    "            'Var1_6':row['Var1_6'],\n",
    "            'Var1_7':row['Var1_7'],\n",
    "            'Var1_8':row['Var1_8'],\n",
    "            'Var1_9':row['Var1_9'],\n",
    "            'Var1_10':row['Var1_10'],\n",
    "            'Var1_11':row['Var1_11'],\n",
    "            'Var1_12':row['Var1_12'],\n",
    "            'Var1_13':row['Var1_13'],\n",
    "            'Var1_14':row['Var1_14'],\n",
    "            'Var1_15':row['Var1_15'],\n",
    "            'Var1_16':row['Var1_16'],\n",
    "            'Var1_17':row['Var1_17'],\n",
    "            'Var1_18':row['Var1_18'],\n",
    "            'Var1_19':row['Var1_19'],\n",
    "            'Var1_20':row['Var1_20'],\n",
    "            'Label':1\n",
    "        }\n",
    "        dataset_positive.append(temp)\n",
    "\n",
    "for _ in range(50):\n",
    "    for _,row in positive.iterrows():\n",
    "        temp={\n",
    "            'Var1_1':row['Var1_1']+np.random.uniform(vars1p),\n",
    "            'Var1_2':row['Var1_2']+np.random.uniform(vars2p),\n",
    "            'Var1_3':row['Var1_3']+np.random.uniform(vars3p),\n",
    "            'Var1_4':row['Var1_4']+np.random.uniform(vars4p),\n",
    "            'Var1_5':row['Var1_5']+np.random.uniform(vars5p),\n",
    "            'Var1_6':row['Var1_6']+np.random.uniform(vars6p),\n",
    "            'Var1_7':row['Var1_7']+np.random.uniform(vars7p),\n",
    "            'Var1_8':row['Var1_8']+np.random.uniform(vars8p),\n",
    "            'Var1_9':row['Var1_9']+np.random.uniform(vars9p),\n",
    "            'Var1_10':row['Var1_10']+np.random.uniform(vars10p),\n",
    "            'Var1_11':row['Var1_11']+np.random.uniform(vars11p),\n",
    "            'Var1_12':row['Var1_12']+np.random.uniform(vars12p),\n",
    "            'Var1_13':row['Var1_13']+np.random.uniform(vars13p),\n",
    "            'Var1_14':row['Var1_14']+np.random.uniform(vars14p),\n",
    "            'Var1_15':row['Var1_15']+np.random.uniform(vars15p),\n",
    "            'Var1_16':row['Var1_16']+np.random.uniform(vars16p),\n",
    "            'Var1_17':row['Var1_17']+np.random.uniform(vars17p),\n",
    "            'Var1_18':row['Var1_18']+np.random.uniform(vars18p),\n",
    "            'Var1_19':row['Var1_19']+np.random.uniform(vars19p),\n",
    "            'Var1_20':row['Var1_20']+np.random.uniform(vars20p),\n",
    "            'Label':1\n",
    "        }\n",
    "        dataset_positive.append(temp)\n",
    "\n",
    "print(f'Data size before Performing DataAugmentation:{len(positive)}\\n \\nData size after performing Data Augmentation:{len(dataset_positive)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***making list into dataframe***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_positive1=pd.DataFrame(dataset_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Splitting Negative DATA SAMPLE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative=df.drop(data_aug[(data_aug.Label==1)].index)\n",
    "negative.drop('Label',inplace=True, axis=1)\n",
    "negative.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***DATA AUGMENTATION ON NEGATIVE DATA SAMPLES***\n",
    "##### ***saving each column name with their standard deviation value***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1n=negative.Var1_1\n",
    "var2n=negative.Var1_2\n",
    "var3n=negative.Var1_3\n",
    "var4n=negative.Var1_4\n",
    "var5n=negative.Var1_5\n",
    "var6n=negative.Var1_6\n",
    "var7n=negative.Var1_7\n",
    "var8n=negative.Var1_8\n",
    "var9n=negative.Var1_9\n",
    "var10n=negative.Var1_10\n",
    "var11n=negative.Var1_11\n",
    "var12n=negative.Var1_12\n",
    "var13n=negative.Var1_13\n",
    "var14n=negative.Var1_14\n",
    "var15n=negative.Var1_15\n",
    "var16n=negative.Var1_16\n",
    "var17n=negative.Var1_17\n",
    "var18n=negative.Var1_18\n",
    "var19n=negative.Var1_19\n",
    "var20n=negative.Var1_20\n",
    "labeln=0\n",
    "##############\n",
    "vars1n=np.std(var1n)\n",
    "vars2n=np.std(var2n)\n",
    "vars3n=np.std(var3n)\n",
    "vars4n=np.std(var4n)\n",
    "vars5n=np.std(var5n)\n",
    "vars6n=np.std(var6n)\n",
    "vars7n=np.std(var7n)\n",
    "vars8n=np.std(var8n)\n",
    "vars9n=np.std(var9n)\n",
    "vars10n=np.std(var10n)\n",
    "vars11n=np.std(var11n)\n",
    "vars12n=np.std(var12n)\n",
    "vars13n=np.std(var13n)\n",
    "vars14n=np.std(var14n)\n",
    "vars15n=np.std(var15n)\n",
    "vars16n=np.std(var16n)\n",
    "vars17n=np.std(var17n)\n",
    "vars18n=np.std(var18n)\n",
    "vars19n=np.std(var19n)\n",
    "vars20n=np.std(var20n)\n",
    "labeln=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***performing full data augmentation on NEGATIVE samples***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_negative=[]\n",
    "for _,row in negative.iterrows():\n",
    "        temp={\n",
    "            'Var1_1':row['Var1_1'],\n",
    "            'Var1_2':row['Var1_2'],\n",
    "            'Var1_3':row['Var1_3'],\n",
    "            'Var1_4':row['Var1_4'],\n",
    "            'Var1_5':row['Var1_5'],\n",
    "            'Var1_6':row['Var1_6'],\n",
    "            'Var1_7':row['Var1_7'],\n",
    "            'Var1_8':row['Var1_8'],\n",
    "            'Var1_9':row['Var1_9'],\n",
    "            'Var1_10':row['Var1_10'],\n",
    "            'Var1_11':row['Var1_11'],\n",
    "            'Var1_12':row['Var1_12'],\n",
    "            'Var1_13':row['Var1_13'],\n",
    "            'Var1_14':row['Var1_14'],\n",
    "            'Var1_15':row['Var1_15'],\n",
    "            'Var1_16':row['Var1_16'],\n",
    "            'Var1_17':row['Var1_17'],\n",
    "            'Var1_18':row['Var1_18'],\n",
    "            'Var1_19':row['Var1_19'],\n",
    "            'Var1_20':row['Var1_20'],\n",
    "            'Label':0\n",
    "        }\n",
    "        dataset_negative.append(temp)\n",
    "\n",
    "for _ in range(50):\n",
    "    for _,row in negative.iterrows():\n",
    "        temp={\n",
    "            'Var1_1':row['Var1_1']+np.random.uniform(vars1n),\n",
    "            'Var1_2':row['Var1_2']+np.random.uniform(vars2n),\n",
    "            'Var1_3':row['Var1_3']+np.random.uniform(vars3n),\n",
    "            'Var1_4':row['Var1_4']+np.random.uniform(vars4n),\n",
    "            'Var1_5':row['Var1_5']+np.random.uniform(vars5n),\n",
    "            'Var1_6':row['Var1_6']+np.random.uniform(vars6n),\n",
    "            'Var1_7':row['Var1_7']+np.random.uniform(vars7n),\n",
    "            'Var1_8':row['Var1_8']+np.random.uniform(vars8n),\n",
    "            'Var1_9':row['Var1_9']+np.random.uniform(vars9n),\n",
    "            'Var1_10':row['Var1_10']+np.random.uniform(vars10n),\n",
    "            'Var1_11':row['Var1_11']+np.random.uniform(vars11n),\n",
    "            'Var1_12':row['Var1_12']+np.random.uniform(vars12n),\n",
    "            'Var1_13':row['Var1_13']+np.random.uniform(vars13n),\n",
    "            'Var1_14':row['Var1_14']+np.random.uniform(vars14n),\n",
    "            'Var1_15':row['Var1_15']+np.random.uniform(vars15n),\n",
    "            'Var1_16':row['Var1_16']+np.random.uniform(vars16n),\n",
    "            'Var1_17':row['Var1_17']+np.random.uniform(vars17n),\n",
    "            'Var1_18':row['Var1_18']+np.random.uniform(vars18n),\n",
    "            'Var1_19':row['Var1_19']+np.random.uniform(vars19n),\n",
    "            'Var1_20':row['Var1_20']+np.random.uniform(vars20n),\n",
    "            'Label':0\n",
    "        }\n",
    "        dataset_negative.append(temp)\n",
    "\n",
    "\n",
    "print(f'Data size before Performing DataAugmentation:{len(negative)}\\n \\nData size after performing Data Augmentation:{len(dataset_negative)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***making list into dataframe***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_negative1=pd.DataFrame(dataset_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***MERGING POSITIVE AND NEGATIVE DATA SAMPLES INTO ONE DATASETS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_full=dataset_positive1.append(dataset_negative1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***SPLITTING FINAL DATASET INTO INPUT (X) AND OUTPUT (Y)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final=dataset_full.iloc[:,0:20]\n",
    "Y_final=dataset_full.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>***Passing Data to Machine Learning Algorithms***</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Implementing Decision Tree Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X_final,Y_final,test_size=0.25)\n",
    "model = DecisionTreeClassifier(random_state=20).fit(X_train,Y_train)\n",
    "Y_predicted = model.predict(X_test)\n",
    "# in line 5 Y_test means Y_true\n",
    "score = accuracy_score(Y_test,Y_predicted)\n",
    "print(f'the accuracy score is:{score}')\n",
    "f1 = f1_score(Y_test,Y_predicted)\n",
    "print(f'the f1-score is:{f1}')\n",
    "rcl = recall_score(Y_test,Y_predicted)\n",
    "print(f'the recall-score is:{rcl}')\n",
    "con_matrix = confusion_matrix(Y_test,Y_predicted)\n",
    "print(f'the confusion_matrix is:{con_matrix}')\n",
    "tn, fp, fn, tp = con_matrix.ravel() # ravel is used to flatten returns contiguous flattened array\n",
    "specificity = tn / (tn+fp)\n",
    "print(f'Specificity is:',specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***ROC_Curve for Decision Tree***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(Y_test,  y_pred_proba)\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Implementing Random Forest Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=50).fit(X_train,Y_train)\n",
    "Y_predicted = model.predict(X_test)\n",
    "# in line 5 Y_test means Y_true\n",
    "score = accuracy_score(Y_test,Y_predicted)\n",
    "print(f'the accuracy score is:{score}')\n",
    "f1 = f1_score(Y_test,Y_predicted)\n",
    "print(f'the f1-score is:{f1}')\n",
    "rcl = recall_score(Y_test,Y_predicted)\n",
    "print(f'the recall-score is:{rcl}')\n",
    "con_matrix = confusion_matrix(Y_test,Y_predicted)\n",
    "print(f'the confusion_matrix is:{con_matrix}')\n",
    "tn, fp, fn, tp = con_matrix.ravel() # ravel is used to flatten returns contiguous flattened array\n",
    "specificity = tn / (tn+fp)\n",
    "print(f'Specificity is:',specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***ROC_Curve for Random Forest***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(Y_test,  y_pred_proba)\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Implementing Logistic Regression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X_input,Y_output,test_size=0.2)\n",
    "model = LogisticRegression(C=10,penalty='l2',tol=0.1).fit(X_train,Y_train)\n",
    "Y_predicted = model.predict(X_test)\n",
    "# in line 5 Y_test means Y_true\n",
    "score = accuracy_score(Y_test,Y_predicted)\n",
    "print(f'the accuracy score is:{score}')\n",
    "f1 = f1_score(Y_test,Y_predicted)\n",
    "print(f'the f1-score is:{f1}')\n",
    "#Recall (aka Sensitivity)\n",
    "rcl = recall_score(Y_test,Y_predicted)\n",
    "print(f'the recall-score is:{rcl}')\n",
    "con_matrix = confusion_matrix(Y_test,Y_predicted)\n",
    "print(f'the confusion_matrix is:{con_matrix}')\n",
    "tn, fp, fn, tp = con_matrix.ravel() # ravel is used to flatten returns contiguous flattened array\n",
    "specificity = tn / (tn+fp)\n",
    "print(f'Specificity is:',specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***ROC_Curve for Logistic Reggrestion***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(Y_test,  y_pred_proba)\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Implementing Support Vector Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf',degree=8,  C=1000,gamma=1000000,probability=True).fit(X_train,Y_train)\n",
    "Y_predicted = model.predict(X_test)\n",
    "# in line 5 Y_test means Y_true\n",
    "score = accuracy_score(Y_test,Y_predicted)\n",
    "print(f'the accuracy score is:{score}')\n",
    "f1 = f1_score(Y_test,Y_predicted)\n",
    "print(f'the f1-score is:{f1}')\n",
    "#Recall (aka Sensitivity)\n",
    "rcl = recall_score(Y_test,Y_predicted)\n",
    "print(f'the recall-score is:{rcl}')\n",
    "con_matrix = confusion_matrix(Y_test,Y_predicted)\n",
    "print(f'the confusion_matrix is:{con_matrix}')\n",
    "tn, fp, fn, tp = con_matrix.ravel() # ravel is used to flatten returns contiguous flattened array\n",
    "specificity = tn / (tn+fp)\n",
    "print(f'Specificity is:',specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***ROC_Curve for SVC***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(Y_test,  y_pred_proba)\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Implementing K Neighbour Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = KNeighborsClassifier(n_neighbors=1,weights=\"uniform\",).fit(X_train,Y_train)\n",
    "Y_predicted = model.predict(X_test)\n",
    "# in line 5 Y_test means Y_true\n",
    "score = accuracy_score(Y_test,Y_predicted)\n",
    "print(f'the accuracy score is:{score}')\n",
    "f1 = f1_score(Y_test,Y_predicted)\n",
    "print(f'the f1-score is:{f1}')\n",
    "#Recall (aka Sensitivity)\n",
    "rcl = recall_score(Y_test,Y_predicted)\n",
    "print(f'the recall-score is:{rcl}')\n",
    "con_matrix = confusion_matrix(Y_test,Y_predicted)\n",
    "print(f'the confusion_matrix is:{con_matrix}')\n",
    "tn, fp, fn, tp = con_matrix.ravel() # ravel is used to flatten returns contiguous flattened array\n",
    "specificity = tn / (tn+fp)\n",
    "print(f'Specificity is:',specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Implementing XG Boost***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X_input,Y_output,test_size=0.2)\n",
    "model = XGBClassifier().fit(X_train,Y_train)\n",
    "Y_predicted = model.predict(X_test)\n",
    "# in line 5 Y_test means Y_true\n",
    "score = accuracy_score(Y_test,Y_predicted)\n",
    "print(f'the accuracy score is:{score}')\n",
    "f1 = f1_score(Y_test,Y_predicted)\n",
    "print(f'the f1-score is:{f1}')\n",
    "rcl = recall_score(Y_test,Y_predicted)\n",
    "print(f'the recall-score is:{rcl}')\n",
    "con_matrix = confusion_matrix(Y_test,Y_predicted)\n",
    "print(f'the confusion_matrix is:{con_matrix}')\n",
    "tn, fp, fn, tp = con_matrix.ravel() # ravel is used to flatten returns contiguous flattened array\n",
    "specificity = tn / (tn+fp)\n",
    "print(f'Specificity is:',specificity)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***ROC_Curve for XG Bosst***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(Y_test,  y_pred_proba)\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml_fyp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62d3cb2c8bfd2224b780a13a6a060ff5c18e02f2a50d55ffa2cb3543640c4d99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
